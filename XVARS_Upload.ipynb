{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5a52ee",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis with Custom, Complex Multi-Variate Distributions\n",
    "\n",
    "Notebook developed by Banamali Panigrahi and Saman Razavi \n",
    "\n",
    "## For the X-VARS method, please cite: \n",
    "Panigrahi, B., Razavi, S., Doig, L. E., Cordell, B., Gupta, H. V., & Liber, K. (2024). On Robustness of the Explanatory Power of Machine Learning Models: Insights from a New Explainable AI Approach using Sensitivity Analysis. Water Resources Research, XX(XXX), e2024WR0XXXX. https://doi.org/10.XXXX/2024WR0XXXX\n",
    "\n",
    "# Example: SA of the Machine Learning Models When Inputs Follow Complex Distribution\n",
    "\n",
    "## Objective:\n",
    "This notebook runs a sensitivity analysis when the theoretical distribution of some inputs is unknown or too complex. Here, the G-VARS (By Do and razavi, 2020) is further extended to accommodate any complex distribution of data that often encountered with environmental systems.\n",
    "\n",
    "### Key Points\n",
    "(1) accounts for complex multi-variate distributional properties of the input-output data, commonly observed with environmental systems\n",
    "(2) offers a global assessment of the input-output response surface formed by machine learning (ML), rather than focusing solely on local regions around existing data points, and \n",
    "(3) is scalable and independent of data size, providing computational efficiency when dealing with large datasets\n",
    "\n",
    "Here, for demonstration purposes,the X-VARS test function for Light gradient boosting (LgBoost) model is used. But to answer the above questions, it can be replaced with any other machine-learning models.\n",
    "\n",
    "First import the required libraries, including GVARS and machine learning model libraries for creating a wrapper around the desired model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95667812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from varstool import GVARS, Model\n",
    "from pandas.core.frame import DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a911d",
   "metadata": {},
   "source": [
    "## Libraries for all connectionist, Kernel, and Tree based ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Seed numbers\n",
    "random.seed(124321)\n",
    "np.random.seed(124321)\n",
    "tf.random.set_seed(124321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf02fd0",
   "metadata": {},
   "source": [
    "# Preapare the input-target dataset matrix and load data in .CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3eec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset in the .csv file are arranged so as to keep the target in the last column\n",
    "Data = pd.read_csv('path/test_data.csv')                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here inputs are from o to 12th Column and 13th colum is the output  \n",
    "X = Data.iloc[:, 0:13].to_numpy()\n",
    "y = Data.iloc[:, 13].to_numpy().reshape(-1, 1)\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2405076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "#correlation_matrix = Data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587961b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_scaled = sc_X.fit_transform(X_train)\n",
    "y_scaled = sc_y.fit_transform(y_train.reshape(-1,1))\n",
    "\n",
    "X_scaled_test = sc_X.fit_transform(X_test)\n",
    "y_scaled_test = sc_y.fit_transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Model to the training dataset\n",
    "regressor_lgboost = LGBMRegressor()\n",
    "regressor_lgboost.fit(X_scaled, y_scaled.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgboost model is used for prediction of the target variable using the train and test set\n",
    "# And converting to original data for calculating the prediction performance metrices\n",
    "\n",
    "y_pred_train = sc_y.inverse_transform(regressor_lgboost.predict(X_scaled).reshape(-1, 1))\n",
    "y_pred_test = sc_y.inverse_transform(regressor_lgboost.predict(X_scaled_test).reshape(-1, 1))\n",
    "\n",
    "# Prediction scores for the Train and Test set results\n",
    "r2_score_train = r2_score(y_train, y_pred_train)\n",
    "r2_score_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Predicting RMSE \n",
    "rmse_train = (np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "rmse_test = (np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Predicting MSE \n",
    "mse_train = (mean_squared_error(y_train, y_pred_train))\n",
    "mse_test = (mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# Predicting MAE \n",
    "mae_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print('R2_score (train): ', r2_score_train)\n",
    "print('R2_score (test): ', r2_score_test)\n",
    "print(\"RMSE (train): \", rmse_train)\n",
    "print(\"RMSE (test): \", rmse_test)\n",
    "print(\"MSE (train): \", mse_train)\n",
    "print(\"MSE (test): \", mse_test)\n",
    "print(\"MAE (train): \", mae_train)\n",
    "print(\"MAE (test): \", mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e4ed1",
   "metadata": {},
   "source": [
    "# Introduce the LgBoost model function for sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11765abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a model is defined to apply the trained/fitted lgboost model with the available dataset callable inside the XVARS framework\n",
    "def LGBMRegressorTest(x):\n",
    "    x_scaled = sc_X.transform(np.array(x).reshape(1, -1))\n",
    "    y_pred_lgboost = regressor_lgboost.predict(x_scaled)\n",
    "    y_pred_lgboost = y_pred_lgboost.astype(float)\n",
    "    y_pred_original = sc_y.inverse_transform(y_pred_lgboost.reshape(1, -1))\n",
    "    return y_pred_original[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ead31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgboost_model = Model(LGBMRegressorTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a93c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.Series({#name  #value\n",
    "             'WT'   : 18.91 ,\n",
    "             'WL'   : 341.72 ,\n",
    "             'EC'   : 2464.8 ,\n",
    "             'Turb'   : 21.59 ,\n",
    "             'RF' : 7.50 ,\n",
    "             'WS' : 7.0 ,\n",
    "             'Rad' : 839.0 ,\n",
    "             'Pres' : 60.68 ,\n",
    "             'AT' : 40.30 ,\n",
    "             'RH' : 100.00 ,\n",
    "             'NH4' : 12.304 ,\n",
    "             'Chl' : 15.60 ,\n",
    "              'pH' : 8.78 ,\n",
    "             })\n",
    "lgboost_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823d5f9",
   "metadata": {},
   "source": [
    "# Set up a X-VARS experiment with custom distributions\n",
    "\n",
    "Create a G-VARS experiment and set its attributes. Please refer to Do and Razavi, 2020 for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc39dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Experiment 1\n",
    "my_parameters = { 'WT': (None, None, None, 'custom'),\n",
    "                 'WL': (None, None, None, 'custom'),\n",
    "                 'EC': (None, None, None, 'custom'), \n",
    "                 'Turb': (None, None, None, 'custom'),\n",
    "                 'RF': (None, None, None, 'custom'),\n",
    "                 'WS': (None, None, None, 'custom'),\n",
    "                 'Rad': (None, None, None, 'custom'),\n",
    "                 'Press': (None, None, None, 'custom'),\n",
    "                 'AT': (None, None, None, 'custom'),\n",
    "                 'RH': (None, None, None, 'custom'),\n",
    "                 'NH4': (None, None, None, 'custom'), \n",
    "                 'Chl': (None, None, None, 'custom'),\n",
    "                 'pH': (None, None, None, 'custom'),\n",
    "                  }\n",
    "my_corr_mat = np.array([\n",
    "                        [1.00,-0.37,0.38,-0.32,-0.04,-0.09,0.21,-0.21,0.56,-0.04,-0.27,0.18,0.40],\n",
    "                        [-0.37,1.00,0.58,0.31,0.07,0.04,-0.15,0.94,-0.13,0.15,0.39,0.48,-0.96],\n",
    "                        [0.38,0.58,1.00,0.00,0.06,0.01,0.10,0.68,0.37,0.02,0.19,0.54,-0.55],\n",
    "                        [-0.32,0.31,0.00,1.00,0.07,0.14,-0.14,0.27,-0.23,0.17,0.29,0.06,-0.25],\n",
    "                        [-0.04,0.07,0.06,0.07,1.00,0.03,-0.05,0.12,-0.03,0.07,0.10,0.01,-0.09],\n",
    "                        [-0.09,0.04,0.01,0.14,0.03,1.00,0.40,0.09,0.18,-0.40,-0.02,-0.11,-0.03],\n",
    "                        [0.21,-0.15,0.10,-0.14,-0.05,0.40,1.00,-0.03,0.59,-0.67,-0.14,-0.13,0.13],\n",
    "                        [-0.21,0.94,0.68,0.27,0.12,0.09,-0.03,1.00,0.07,0.07,0.37,0.54,-0.91],\n",
    "                        [0.56,-0.13,0.37,-0.23,-0.03,0.18,0.59,0.07,1.00,-0.65,-0.21,0.10,0.18],\n",
    "                        [-0.04,0.15,0.02,0.17,0.07,-0.40,-0.67,0.07,-0.65,1.00,0.24,0.13,-0.13],\n",
    "                        [-0.27,0.39,0.19,0.29,0.10,-0.02,-0.14,0.37,-0.21,0.24,1.00,0.08,-0.39],\n",
    "                        [0.18,0.48,0.54,0.06,0.01,-0.11,-0.13,0.54,0.10,0.13,0.08,1.00,-0.51],\n",
    "                        [0.40,-0.96,-0.55,-0.25,-0.09,-0.03,0.13,-0.91,0.18,-0.13,-0.39,-0.51,1.00],\n",
    "                       ])\n",
    "my_num_dir_samples = 10\n",
    "my_delta_h = 0.1 # my_delta_h = 1 / my_num_dir_samples or choose values such as 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfb811",
   "metadata": {},
   "source": [
    "# Define the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1 = GVARS(parameters      = my_parameters,\n",
    "                    corr_mat         = my_corr_mat,\n",
    "                    num_stars        = 100,\n",
    "                    num_dir_samples  = my_num_dir_samples,\n",
    "                    delta_h          = my_delta_h,\n",
    "                    ivars_scales     = (0.1, 0.3, 0.5),\n",
    "                    sampler          = 'lhs',\n",
    "                    slice_size       = 10,\n",
    "                    model            = lgboost_model,\n",
    "                    seed             = 123456789,\n",
    "                    bootstrap_flag   = True,\n",
    "                    bootstrap_size   = 100,\n",
    "                    bootstrap_ci     = 0.9,\n",
    "                    grouping_flag    = False,\n",
    "                    num_grps         = 2,\n",
    "                    dist_sample_file = 'path/test_data.csv',\n",
    "                    fictive_mat_flag = True,\n",
    "                    report_verbose   = True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdc98b",
   "metadata": {},
   "source": [
    "# Run the X-VARS experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1.run_online()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f76c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the lgboost model run output for the target variable column\n",
    "#This displays the all the resampled rows i.e., 10*100*13=13000 rows for the 14 columns comprising inputs-target \n",
    "experiment_1.model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52819b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the martix of X-VARS resampled variables\n",
    "np.savetxt('path\\output.csv',\n",
    "           (experiment_1.model_df), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed967c6",
   "metadata": {},
   "source": [
    "# Check out the results\n",
    "#When the X-VARS analysis is completed, check out the results of sensitivity analysis.\n",
    "\n",
    "#IVARS: Integrated variogram Across a Range of Scales\n",
    "\n",
    "#IVARS indices are the primary sensitivity indices by the VARS approach. First, print all the IVARS indices for the scale ranges (0.1 or 0.3 or 0.5) #of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVARS from Experiment 1\n",
    "cols = experiment_1.parameters.keys()\n",
    "experiment_1.ivars[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6401824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the IVARS values for all the inputs (to decide the ranks of the influential input parameters)\n",
    "np.savetxt('path\\IVARS.csv', \n",
    "           (experiment_1.ivars[cols]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose a scale range and plot the respective IVARS indices. Two points:\n",
    "\n",
    "#POINT1: IVARS-50 (h=[0-0.5]), called ***Total-Variogram Effect*** is the most comprehensive sensitivity index.\n",
    "#POINT2: Plotting sensitivity results in log scale helps us better differentiate less influential parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58109c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot IVARS from Experiment 1\n",
    "ivars_scale = 0.5 # Choose the scale range of interest, e.g., 0.1, 0.3, or 0.5\n",
    "\n",
    "cols = experiment_1.parameters.keys()                     \n",
    "fig_bar = plt.figure(figsize=(10,5))\n",
    "plt.gca().bar(cols, experiment_1.ivars.loc[pd.IndexSlice[ ivars_scale ]][cols], color='gold')\n",
    "plt.gca().set_title (r'Integrated variogram Across a Range of Scales (LgBoost_DO_124321)', fontsize = 15)\n",
    "plt.gca().set_ylabel(r'IVARS-50 (Total-Variogram Effect)', fontsize = 13)\n",
    "plt.gca().set_xlabel(r'Model Parameter', fontsize=13)\n",
    "plt.gca().grid()\n",
    "plt.gca().set_yscale('linear')\n",
    "\n",
    "fig_bar = plt.figure(figsize=(10,5))\n",
    "plt.gca().bar(cols, experiment_1.ivars.loc[pd.IndexSlice[ ivars_scale ]][cols], color='gold')\n",
    "plt.gca().set_title (r'Integrated variogram Across a Range of Scales $[log-scale]$ (LgBoost_DO_124321)', fontsize = 15)\n",
    "plt.gca().set_ylabel(r'IVARS-50 (Total-Variogram Effect)', fontsize = 13)\n",
    "plt.gca().set_xlabel(r'Model Parameter', fontsize=13)\n",
    "plt.gca().grid()\n",
    "plt.gca().set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c8747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
